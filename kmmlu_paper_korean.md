# KMMLU: 한국어 대규모 멀티태스크 언어 이해도 측정

**저자**: 손귀진¹,²,³, 이한울²,⁴, 김성동⁵,⁶, 김승원⁶,⁷, Niklas Muennighoff⁸, 최태균⁵, 박천복⁵, 유강민⁵, Stella Biderman²

**소속**: 연세대학교¹, EleutherAI², OnelineAI³, NCSOFT AI⁴, 네이버 클라우드⁵, KAIST AI⁶, 카네기 멜론 대학교⁷, Contextual AI⁸

**연락처**: spthsrbwls123@yonsei.ac.kr

## 초록

우리는 인문학부터 STEM까지 45개 과목에 걸친 35,030개의 전문가 수준 객관식 문제로 구성된 새로운 한국어 벤치마크인 KMMLU를 제안한다. 기존의 한국어 벤치마크가 영어 벤치마크를 번역한 것인 반면, KMMLU는 한국어 원본 시험에서 수집되어 한국어의 언어적, 문화적 측면을 포착한다. 우리는 27개의 공개 및 독점 LLM을 테스트했으며, 최고의 공개 모델이 50.5%를 기록하여 개선의 여지가 크게 남아있음을 관찰했다. 이 모델은 주로 영어와 중국어를 위해 학습되었으며 한국어를 위한 것은 아니었다. POLYGLOT-KO와 같은 한국어에 특화된 현재 LLM들은 훨씬 못한 성능을 보인다. 놀랍게도 GPT-4와 HYPERCLOVA X와 같은 가장 유능한 독점 LLM조차도 60%를 넘지 못한다. 이는 한국어를 위한 LLM 개선 작업이 더 필요함을 시사하며, KMMLU가 이러한 진전을 추적하는 적절한 도구를 제공한다고 믿는다. 우리는 데이터셋을 Hugging Face Hub에 공개하고 벤치마크를 EleutherAI의 Language Model Evaluation Harness에 통합했다.

## 1. 서론

최근 연구들은 종종 MMLU(Hendrycks et al., 2020)의 번역 버전을 활용하여 대규모 언어 모델(LLM)의 다국어 능력을 평가한다(OpenAI, 2023; Qwen, 2024; Chen et al., 2023; Zhao et al., 2024). 그러나 영어 벤치마크를 단순히 관심 언어로 번역하는 것은 중요한 한계에 직면한다. 첫째, 기계 번역은 부자연스러운 언어, 오타, 문법 오류 등의 문제로 손상된 데이터셋을 초래할 수 있다(Xia et al., 2019; Riley et al., 2023; Yao et al., 2023). 둘째, 주로 영어 화자를 위해 설계된 MMLU는 미국 법률 체계에 대한 지식을 가정하거나 영어 속어와 문화에 대한 친숙함을 요구하는 내용을 포함한다(Lee et al., 2023; Jin et al., 2023; Son et al., 2023; Li et al., 2023a; ZaloAI-JAIST, 2023). 따라서 번역된 버전은 다국어 숙련도를 암시하지만, 원어민이 중요하다고 여길 수 있는 언어적 또는 문화적 측면을 포착하지 못하는 경우가 많다.

한국 NLP 커뮤니티를 위해 이 문제를 해결하기 위해, 우리는 45개 과목에 걸친 35,030개의 질문으로 구성된 포괄적인 벤치마크인 KMMLU를 소개한다. KMMLU의 고유한 특징은 그 출처이다: 모든 질문은 한국어 시험에서 유래되어 번역된 자료 없이 진정한 한국어를 보장한다. 또한 우리의 질문은 한국에 현지화되어 있다: 서구인이 아닌 한국인의 문화적 태도를 반영한다(그림 1 참조). 그림 4에 묘사된 우리의 비교 분석은 KMMLU가 언어적으로 자연스럽고 한국 문화적 맥락에 깊이 뿌리박힌 질문을 제공함으로써 이전의 번역된 벤치마크를 능가함을 보여준다. 번역(Park et al., 2024; Ham et al., 2020; Jin et al., 2023) 또는 비공개 데이터셋(Park et al., 2024, 2021; Lee et al., 2024)에 크게 의존하는 한국어 벤치마킹의 단점을 해결하기 위해, 우리는 KMMLU¹과 해당 평가 코드²를 공개한다.

우리는 5개 범주에 걸쳐 27개의 다른 LLM을 평가한다: (1) 다국어 사전학습 모델(Touvron et al., 2023; Young et al., 2024; Bai et al., 2023); (2) 다국어 채팅 모델; (3) 한국어 사전학습 모델(Ko et al., 2023); (4) 한국어 지속 사전학습 모델(L. Junbum, 2023b); (5) 한국에서 서비스되는 모델을 포함한 독점 모델(OpenAI, 2023; Team et al., 2023; Yoo et al., 2024). 우리의 결과는 GPT-4가 59.95%로 가장 높은 점수를 기록하며 상당한 개선 여지를 보여준다. 놀랍게도, BLOOM(Workshop et al., 2022)을 단일언어 영어 모델(Biderman et al., 2023; Peng et al., 2023)과 비교한 이전 연구에서 논의된 "다국어성의 저주"(Conneau et al., 2019; Pfeiffer et al., 2022)에 대한 증거를 거의 발견하지 못했다.

마지막으로, LLM이 질문 답변에서 한국어 지식을 어떻게 활용하는지 더 잘 이해하기 위해 상세한 분석을 수행한다. 처음에, GPT-4의 전반적인 우수성에도 불구하고 현지화된 지식을 요구하는 영역에서 주목할 만한 격차를 보이며 벤치마크 현지화의 중요성을 보여준다는 것을 관찰한다. 예를 들어, 한국사에서 GPT-4(OpenAI, 2023)는 한국어 특화 LLM인 HYPERCLOVA X(Yoo et al., 2024)의 44%에 비해 35% 성공률을 달성한다. 특히, HYPERCLOVA X는 Chain-of-Thought(CoT) 프롬프팅 사용으로 일관된 개선을 보이는 유일한 모델로, 비한국어 LLM이 정확하고 신뢰할 수 있는 한국어 설명을 생성하는 데 직면하는 도전을 나타낸다.

---

¹ https://huggingface.co/datasets/HAERAE-HUB/KMMLU  
² github.com/EleutherAI/lm-evaluation-harness/tree/main/lm_eval/tasks/kmmlu

## 2. 관련 연구

### 2.1 대규모 언어 모델을 위한 벤치마크

벤치마크는 LLM의 진화하는 능력을 정확하게 이해하고 추적하는 데 필수적이다. 전통적으로 벤치마크는 언어 작업에 초점을 맞췄지만(Rajpurkar et al., 2016; Wang et al., 2019b,a), 최근 더 유능한 LLM의 급증으로 이러한 접근법은 구식이 되었다. 이 격차를 해결하기 위해 상식 추론(Clark et al., 2018; Sakaguchi et al., 2021; Zellers et al., 2019), 수학적 추론(Hendrycks et al., 2021; Cobbe et al., 2021), 코드 생성(Chen et al., 2021; Li et al., 2023b), 다중 턴 대화(Zheng et al., 2023)와 같은 고수준 능력에 초점을 맞춘 새로운 벤치마크가 등장했다. 특히, 일부 노력은 광범위한 지식 기반 주제를 다루는 방대한 데이터셋을 통해 능력을 평가하는 데 집중했다(Hendrycks et al., 2020; Srivastava et al., 2022; Sawada et al., 2023). 가장 유명한 MMLU(Massive Multitask Language Understanding)(Hendrycks et al., 2020)는 기초 수학부터 법률 및 컴퓨터 과학과 같은 복잡한 영역까지 57개 과목에 걸쳐 다양한 분야에서 LLM을 평가한다. 이러한 노력의 대부분이 주로 영어에 초점을 맞췄지만, 다른 언어를 위해 유사한 벤치마크를 적응하고 생성하는 진전이 있었다(Li et al., 2023a; Huang et al., 2023; Zeng, 2023; Ghahroodi et al., 2024; Koto et al., 2024, 2023; Tam et al., 2024).

### 2.2 한국어 벤치마크

**표 1: 한국어 벤치마크 개요.** "# Instances"는 테스트 세트의 크기를 나타낸다. KLUE 데이터셋은 테스트 세트를 공개하지 않았으므로 공개로 간주하지 않는다. KORNAT 데이터셋의 완전한 공개는 향후 예정되어 있으며 현재는 △로 표시된 대로 사용할 수 없다. KoBBQ의 일부는 번역되었다.

| 한국어 벤치마크 | 번역됨 | 공개? | # 범주 | # 인스턴스 | 유형 |
|---|---|---|---|---|---|
| KORNLI & KORSTS | O | O | 2 | 6,389 | 이해 |
| KLUE | X | X | 8 | 38,500 | 이해 |
| KO-H5 | O | X | 5 | 25,700 | 추론 |
| KOBBQ | △ | O | 268 | 76,048 | 편향 |
| HAE-RAE BENCH. | X | O | 6 | 1,538 | 문화 지식 |
| CLICK | X | O | 11 | 1,995 | 문화 지식 |
| KORNAT | X | △ | 16 | 10,000 | 문화 정렬 |
| KMMLU (우리) | X | O | 45 | 35,030 | 전문 지식 & 추론 |

한국어를 위한 이전 벤치마크는 자연어 추론(Ham et al., 2020), 기계 독해(Lim et al., 2019), 혐오 발화 탐지(Moon et al., 2020)를 포함한 자연어 이해 작업에 특화되었다. General Language Understanding Evaluation(GLUE)(Wang et al., 2019b)과 유사한 Korean Language Understanding Evaluation(KLUE) 벤치마크(Park et al., 2021)는 한국어 이해를 측정하기 위한 8개의 하위 작업을 컴파일했다. 그럼에도 불구하고 이러한 벤치마크는 추론 능력 평가에 한계를 보여 LLM 평가에 불충분했다. Ko-H5(Park et al., 2024)는 HellaSwag(Zellers et al., 2019), MMLU(Hendrycks et al., 2020), ARC(Clark et al., 2018), TruthfulQA(Lin et al., 2021)와 같은 광범위한 추론 벤치마크를 제공함으로써 이러한 단점을 극복하고자 한다. 그럼에도 불구하고, 그들은 오류에 취약한 기계/인간 번역에 의존한다. 더욱이, 그들의 데이터셋은 비공개이며 140억 매개변수보다 큰 모델에 대한 평가를 제공하지 않아 투명성과 유용성이 제한된다.

한국어를 위해 개발된 최근 벤치마크는 원래 영어 벤치마크의 단순한 번역을 넘어 언어적, 문화적 뉘앙스를 보존하는 데 초점을 옮겼다. 이러한 맥락에서 Jin et al. (2023)은 원래 BBQ 데이터셋(Parrish et al., 2022)에서 파생된 Korean Bias Benchmark for Question Answering을 개발했다. 이 벤치마크는 한국 문화적 맥락에 맞게 특별히 조정되었다. 또한 미묘한 문화적 뉘앙스를 포착하기 위해 처음부터 진정한 한국어 데이터셋을 만들려는 노력이 있었다. 예를 들어, HAE-RAE Benchmark(Son et al., 2023)와 CLIcK(Kim et al., 2024)는 한국에 특정한 문화적, 언어적 이해를 평가한다. 또한 KorNAT(Lee et al., 2024)는 한국의 국가적 특성과의 정렬을 구체적으로 평가하기 위해 도입되었으며, 사회적 가치와 일반 지식이라는 두 가지 핵심 측면에 초점을 맞춘다.

KMMLU 벤치마크는 생물학, 화학, 형법, 세법, 전기공학, 항공공학 등을 포함하여 광범위하고 깊은 분야를 다루는 최초의 광범위한 지식 벤치마크로서 이전 노력을 넘어선다. 또한 데이터셋, 프롬프트, 평가 코드가 공개적으로 사용 가능하여 투명하고 재현 가능한 평가를 보장한다는 점에서 독특하다. 중요한 것은, 전적으로 한국어 시험에서 구축되어 한국 문화와 언어적 뉘앙스의 표현을 제공한다는 것이다.

## 3. KMMLU

### 3.1 작업 개요

**표 2: KMMLU의 질문 개요:** 이 표는 인간 응시자를 위한 전제 조건 수, 질문에 부정이 포함되어 있는지 여부, 훈련/검증/테스트 분할에 따라 질문을 요약한다.

| 범주 | # 질문 |
|---|---|
| **전제 조건** |  |
| 없음 | 59,909 |
| 1개 전제 조건 시험 | 12,316 |
| 2개 전제 조건 시험 | 776 |
| 2년 이상 경력 | 65,135 |
| 4년 이상 경력 | 98,678 |
| 9년 이상 경력 | 6,963 |
| **질문 유형** |  |
| 긍정 | 207,030 |
| 부정 | 36,747 |
| **분할** |  |
| 훈련 | 208,522 |
| 검증 | 225 |
| 테스트 | 35,030 |
| **총계** | 243,777 |

KMMLU는 HUMSS(인문사회과학), STEM(과학, 기술, 공학, 수학), 응용과학 및 기타 전문 수준 지식을 포함한 45개 범주에 걸친 35,030개의 객관식 문제 모음이다. STEM 내에서는 자연과학과 물리과학부터 기술 및 공학 분야까지 과학적 원리를 강조하는 주제에 초점을 맞춘다. 한편, 응용과학은 항공공학 및 정비, 가스기술공학, 비파괴검사와 같은 산업별 과목을 포함한다. HUMSS는 역사와 심리학을 포함한 광범위한 과목을 다루며 인간 사회와 문화의 다양한 측면에 대한 심층적인 통찰력을 제공한다. 세 범주 중 어느 것에도 맞지 않는 나머지 과목은 기타로 분류된다.

우리는 주로 한국 자격시험에서 질문을 가져오는데, 특히 KMMLU가 가져오는 일부 자격시험은 최소 9년의 업계 경험이 필요한 시험이다. 또한 KMMLU는 그림 1에 표시된 것처럼 해결하기 위해 문화적, 지역적, 법적 지식의 이해가 필요한 질문을 포함한다. 자세한 내용은 표 2를 참조하라.

### 3.2 데이터셋 생성

우리의 데이터셋은 공무원 적성 검사(PSAT), 한국 자격시험, 대학수학능력시험(CSAT)에 이르는 533개의 다양한 출처에서 수집된 질문의 모음이다. 이 컬렉션은 고등학교부터 전문가 수준까지 광범위한 학문적 스펙트럼을 포함한다.

처음에 자동 크롤링을 사용하여 371,002개의 질문을 수집했다. 그런 다음 불용어, 정규식 패턴, 모델 기반 분류기를 포함하여 중복된 샘플이나 구문 분석 오류를 지우기 위해 휴리스틱 필터를 구현한다. 또한 4개 미만의 옵션이 있는 질문을 제외하고 4개 이상인 질문을 조정하여 형식을 표준화한다. 이러한 필터링으로 데이터셋이 34% 감소하여 243,777개의 질문이 되었다. 데이터셋 크기의 상당한 감소는 두 가지 주요 요인에서 비롯된다: 첫째, 우리는 품질을 양보다 우선시하여 일부 유효한 샘플을 제거하는 비용을 감수하더라도 의심스러운 내용을 제거하기 위해 광범위한 필터를 사용한다. 둘째, 특히 계절적으로 반복되는 질문 중에서 높은 정도의 유사성을 관찰하여 광범위한 중복 제거가 발생한다.

가능한 경우 실제 시험 응시자로부터 인간 정확도 데이터를 수집한다. 우리 데이터셋 시험의 약 90%가 인간 성능 데이터를 포함하며, 평균 정확도는 62.6%이다. 데이터셋의 대부분의 자격시험은 합격하기 위해 80% 점수가 필요하다. PSAT의 경우 지난 5년간 평균 합격 점수는 83.7%였다. 따라서 KMMLU에서 80% 이상을 달성하는 것은 인간 전문가의 최소 성능과 동등한 것으로 간주될 수 있으며, 최고의 전문가는 100%에 가까운 점수를 기록할 가능성이 높다. 데이터셋은 훈련 세트, 소수 샷 개발 세트, 테스트 세트의 세 가지 구성 요소로 구성된다. 소수 샷 개발 세트는 문맥 내 학습을 지원하기 위해 과목당 5개의 질문을 특징으로 한다(Brown et al., 2020). 훈련 세트에는 하이퍼파라미터 튜닝과 모델 훈련 모두에 적합한 208,522개의 질문이 포함되어 있다. 테스트 세트는 인간 정확도가 가장 낮은 질문 모음으로, 각 과목은 최소 100개의 인스턴스로 구성되며 총 35,050개의 질문이 있다. 그러나 테스트 출처와 응시자 집단의 변화로 인해 인간 정확도를 직접 비교하는 것이 바람직하지 않을 수 있다는 점에 유의해야 한다. 일부 그룹은 다른 그룹보다 더 전문적이다. 인간 데이터를 직접 수집하는 것이 잠재적으로 이 문제를 해결할 수 있지만, 예산 제약으로 인해 그렇게 할 수 없었다.

최종 확정 전에 6개월 동안 데이터셋을 대중에게 공개했다. 이 기간 동안 커뮤니티에서 보고된 5개의 문제를 받았고 그에 따라 741개의 인스턴스가 수정되었다. 또한 테스트 세트의 35,030개 질문은 저작권이 있는 자료를 제거하기 위해 수동 검토를 거쳤다. 저작권이 있는 자료를 포함한 147개의 인스턴스를 교체했다. 이 과정에서 추가 오류는 확인되지 않았다. 마지막으로 Xu et al. (2024)의 방법을 기반으로 잠재적인 데이터 유출을 찾기 위한 분석을 수행한다. 공개 및 독점 LLM 모두 KMMLU 벤치마크를 회상하지 못하는 것을 관찰하여 벤치마크 오염 가능성이 낮음을 시사한다. 자세한 내용은 섹션 C를 참조하라.

### 3.3 CoT 예시 생성

Chung et al. (2022)은 MMLU Hendrycks et al. (2020)에 대한 CoT 추론을 테스트하기 위해 5-shot 예시를 고안한다. 마찬가지로, 우리는 벤치마크에서 모델의 추론 능력을 테스트하기 위해 각 과목에 대해 5-shot CoT 예시를 만든다. 그러나 다양한 범위의 전문가 수준 테스트에 대한 정확한 근거를 작성하는 것은 어려운 문제이다. 이상적인 해결책은 각 테스트에 대해 전문가를 초대하는 것이지만, 자원 제약을 고려하여 다양한 LLM의 도움을 활용하기로 결정했다. 구체적으로, 우리는 GPT-4와 HyperCLOVA X라는 두 LLM을 사용하여 zero-shot CoT Kojima et al. (2022)와 브라우징 증강 CoT라는 다양한 프롬프트 기술을 사용한다.

먼저, 두 프롬프트 기술을 사용하여 LLM에서 근거와 해당 답변을 유도한다. 또한 오버샘플링으로 얻은 10개의 추론 경로에 대해 다수결 투표 방법인 self-consistency Wang et al. (2022)를 활용한다. 결과적으로 이 단계는 각 입력에 대해 4 × 10개의 근거를 생성한다(4 = 2개 LLM과 2개 프롬프트 유형). 그런 다음 더 길고 반복이 적은 출력을 기준으로 상위 4개 근거를 선택한다. 마지막으로 저자들은 상위 4개 중에서 가장 적절한 근거를 수동으로 선택하고 필요한 경우 철저한 검사를 통해 수정한다. 품질 관리를 위해 각 질문에 대해 두 명의 작업자를 확보한다. 첫 번째 반복에서 두 작업자 간의 약 87% 일치를 발견한다. 나머지 충돌하는 예제를 반복적으로 검증한다. 총 45 × 5 = 225개의 예시를 벤치마크 내 CoT 추론을 위해 생성한다. 자세한 내용은 부록 H를 참조하라.

### 3.4 KMMLU-HARD

KMMLU는 이전 버전인 MMLU(Hendrycks et al., 2020)와 CMMLU(Li et al., 2023a)를 능가하는 35,030개의 질문으로 구성되어 있다. 따라서 KMMLU 외에도 보다 표적화되고 효율적인 평가를 위해 KMMLU-HARD를 만든다. KMMLU-HARD 하위 집합에는 GPT-3.5 TURBO, GEMINI PRO, HYPERCLOVA X, GPT-4 중 적어도 하나가 올바르게 답변하지 못한 4,104개의 질문이 포함되어 있다. 이러한 질문은 모든 범주에 균등하게 분포되어 있으며 각각 23~100개의 질문을 포함한다.

## 4. 실험 설정

### 4.1 평가 방법론

KMMLU에서 LLM을 평가할 때 포괄적인 비교를 위해 두 가지 다른 설정을 사용한다. 첫째, Direct 방법은 그리디 디코딩을 통해 가장 그럴듯한 옵션을 생성하도록 모델에 프롬프트를 제공한다. 이 과정에서 각 모델은 전체 어휘에서 응답을 생성하므로 1/vocab_size가 무작위 기준선이 된다. 둘째, CoT는 모델이 자유롭게 텍스트를 생성할 수 있도록 하고 RegEx를 활용하여 결과를 구문 분석한다. 최종 답변 전에 추론 시퀀스를 생성함으로써 CoT는 LLM이 추론이 많은 작업을 해결하는 데 도움을 주는 데 성공했다. 모델은 CoT 생성에 그리디 디코딩을 사용하도록 설정되어 있다. 이 논문의 모든 평가는 방법에 관계없이 5개의 예시가 있는 소수 샷 설정에서 수행된다. 하드웨어 제약으로 인해 8비트 양자화를 사용하여 공개 모델로 실험을 실행한다.

### 4.2 모델

우리 연구에서는 전문가 수준의 한국어 질문에 답하는 기존 LLM에 대한 포괄적인 개요를 제공하기 위해 크기, 언어 및 훈련 단계가 다양한 27개 모델을 평가한다.

27개 모델에는 다음이 포함된다:

1. **다국어 사전학습 모델**: LLAMA-2 (7B, 13B, 70B) (Touvron et al., 2023), QWEN (7B, 14B, 72B) (Bai et al., 2023), Yi (6B, 34B) (Young et al., 2024);
2. **다국어 채팅 모델**: LLAMA-2, QWEN, YI의 채팅 버전;
3. **한국어 사전학습 모델**: POLYGLOT-KO (1.3B, 3.8B, 5.8B, 12.8B) (Ko et al., 2023);
4. **한국어 지속 사전학습 모델**: LLAMA-2-KO (L. Junbum, 2023b) (7B), YI-KO (L. Junbum, 2023a) (6B, 34B);
5. **독점 모델**: GPT-3.5/4 (OpenAI, 2023), GEMINI PRO (Team et al., 2023), HYPERCLOVA X Yoo et al. (2024).

영어 및 중국어 이중언어 모델의 포함은 중국어 한자가 한국어에 미친 역사적 영향을 고려할 때 잠재적인 파급 효과를 탐색하는 것을 목표로 한다. 모델에 대한 자세한 내용은 부록 E와 표 14에 제공된다.

## 5. 평가 결과

### 사전학습 계산

표 3에서 Direct 방법을 사용하여 27개 LLM의 성능을 비교한다. 우리는 사전학습 및 미세조정 모델 전반에 걸쳐 명확한 추세를 관찰한다. 더 큰 컴퓨팅 예산을 가진 모델이 우수한 성능을 보인다. 이러한 스케일링 동작은 증가된 컴퓨팅 리소스(매개변수 수와 훈련 코퍼스 크기에 반영됨)가 복잡한 언어 작업을 더 정확하게 처리하는 모델의 능력을 향상시킨다는 것을 나타낸다. 특히, 한국어로만 훈련되었음에도 불구하고 POLYGLOT-KO-12.8B의 성능은 25%의 무작위 기준선을 간신히 초과하고, 영어 중심 LLAMA-2-13B와 동등하며, 유사한 크기의 YI 및 QWEN 모델보다 뒤처진다. 이는 높은 성능을 달성하는 데 있어 긴 훈련 실행의 중요성을 강조한다: POLYGLOT-KO-12.8B는 대략 계산 최적으로 훈련되었지만(Hoffmann et al., 2022), 훈련 데이터 크기의 크기 순서 증가는 이러한 비최적 훈련 모델의 성능을 실질적으로 증가시킨다. 이러한 훈련 리소스의 격차는 그림 5에서 더욱 설명되며, Polyglot-Ko의 상대적으로 낮은 훈련 예산이 명백하다.

### 미세조정

표 3에서 사전학습 모델을 미세조정하는 것이 반드시 더 나은 성능으로 이어지지는 않는다는 것도 관찰한다. 우리 실험에서 모델은 종종 기본 버전과 채팅 버전 간에 작은 성능 차이를 보인다. 이는 지도 미세조정, 직접 선호도 최적화 또는 강화 학습과 같은 미세조정 방법이 언어 모델의 지식에 미미한 개선을 가져온다고 제안한 과거 연구와 일치한다(Bi et al., 2024). 흥미롭게도 QWEN-72B와 LLAMA-2-70B는 각각 -3.55%와 -5.81%의 성능 하락을 경험한다. 우리는 서로 다른 언어의 사전학습 모델에서 한국어 질문을 해결하는 능력이 원래 사전학습 코퍼스에서 한국어 텍스트를 완벽하게 필터링하지 못한 것에서 비롯된다고 의심한다. 그러나 사후 훈련 과정 중에 사용되는 데이터셋은 종종 더 큰 정밀도로 큐레이션되어 모든 비대상 언어를 제외할 수 있다. 따라서 이러한 모델의 한국어 능력을 해칠 수 있다.

### 대규모 다국어성

"다국어성의 저주"(Conneau et al., 2019; Pfeiffer et al., 2022)는 모델이 다국어 코퍼스에서 훈련될 때 모델 능력의 명백한 감소를 나타낸다. 저주는 작은 모델에 심각할 수 있지만, 마스크 언어 모델의 경우 규모에 따라 약화되는 것이 관찰되었다(Goyal et al., 2021; Pfeiffer et al., 2022). 경험적으로, 이는 디코더 전용 BLOOM 모델(Workshop et al., 2022)의 경우가 아닌 것으로 보인다. 여러 논문에서 단일언어 영어 모델이 영어 작업에서 BLOOM을 실질적으로 능가한다는 것을 발견했다(Biderman et al., 2023; Peng et al., 2023). 대조적으로, 우리는 언어 간 긍정적 전이의 증거를 발견한다. LLAMA-2, YI, QWEN과 같은 대규모 다국어 모델이 단일언어 POLYGLOT-KO를 실질적으로 능가한다. 다국어 모델이 POLYGLOT-KO보다 크기 순서만큼 더 많은 토큰으로 훈련되었지만, 사전학습 단계에서 훨씬 적은 한국어 텍스트를 만난다. 예를 들어, LLAMA-2는 2조 토큰으로 훈련되었으며, 한국어는 0.06%에 불과하여 12억 토큰에 해당한다. YI는 중국어와 영어 이외의 언어를 제외하기 위해 언어 필터를 사용하고, QWEN은 데이터의 상당 부분이 영어와 중국어라고 언급한다. 비교하면 POLYGLOT-KO 모델은 모델 크기에 따라 1,670억에서 2,190억 토큰으로 훈련된다. 우리의 결과는 스케일된 디코더 전용 모델이 심각하게 과소 훈련된 언어에서 능력을 획득한다는 것을 보여주며, 이는 이전 연구와 일치하는 발견이다(Muennighoff et al., 2023).

### 지속 사전학습

이전에 언급했듯이 초기에 이중언어(영어 및 중국어) 사용을 위해 훈련된 YI 시리즈 모델은 POLYGLOT-KO 모델을 크게 능가한다. 이러한 성능 격차는 이러한 모델의 지속적인 사전학습으로 더욱 확대된다. 표 3에서 우리는 한국어를 포함하도록 어휘를 확장한 후 각각 추가로 600억 및 400억 토큰으로 지속적으로 훈련된 YI-KO 6B 및 34B 모델을 평가한다. 또한 그림 3에서 YI-KO-34B 모델의 사용 가능한 체크포인트를 분석하여 첫 번째 체크포인트에서 초기 하락 후 일관된 성능 증가를 주목한다. 이 초기 하락은 처음에 훈련을 방해할 수 있는 확장된 어휘 때문일 가능성이 높다(Zhao et al., 2024). 자세한 내용은 섹션 B.1을 참조하라.

## 6. 분석

### 6.1 KMMLU의 한국 특정 인스턴스 분석

KMMLU가 MMLU를 번역한 과거 노력과 어떻게 다른지에 대한 더 깊은 통찰력을 제공하기 위해(Park et al., 2023; Chen et al., 2023), 우리는 두 가지 측면에서 둘을 비교한다: 표현의 자연스러움과 전문화된 한국어 지식의 필요성. 분석을 위해 두 데이터셋 내 각 범주에서 10개의 질문을 무작위로 선택하여 한국어로 번역된 MMLU에서 570개 질문과 KMMLU에서 450개 질문을 얻었다. 두 명의 저자가 각 질문을 평가했다.

그림 4는 두 하위 집합이 한국어 원어민에게 어떻게 나타나는지의 차이를 보여준다. KMMLU 질문은 상당히 더 자연스럽고 문화적으로 관련이 있어 한국어의 뉘앙스와 문화적 특성을 반영하는 데 있어 MMLU의 한계를 강조한다. 미국 시험에서 파생된 MMLU는 본질적으로 한국 문화에 대한 질문이 부족하다. 반대로 KMMLU의 20.4%는 한국 문화 관행, 사회 규범 및 법적 프레임워크를 이해해야 한다. 이러한 격차는 MMLU의 "high_school_government_and_politics"와 같은 범주에서 명백하다. 이는 미국 정부 시스템에 대한 친숙함을 가정하여 미국 중심 콘텐츠에 크게 기울어져 있고, 미국 속어에 대한 지식을 전제로 하는 "miscellaneous" 범주는 데이터셋 내에 포함된 문화적 편향을 강조한다. 그러나 KMMLU 콘텐츠의 20.4%가 문화적으로 특정하지만 나머지는 언어 모델의 일반 지식을 광범위하게 평가한다는 점에 유의하는 것이 중요하다. 여기에는 특정 국가의 지식 기반에 묶이지 않고 보편적으로 적용 가능한 수학과 같은 영역이 포함된다.

### 6.2 Chain-of-Thought 프롬프팅이 KMMLU 성능을 향상시킬 수 있는가?

우리는 고급 프롬프팅 방법이 성능을 향상시킬 수 있는지 조사하기 위해 5-shot 예시(섹션 3.3)를 활용하여 few-shot CoT 프롬프팅 Wei et al. (2022)을 사용한다. CoT 프롬프팅은 Direct 방법보다 훨씬 긴 시퀀스 생성을 필요로 하므로 리소스 제약을 고려하여 KMMLU-Hard 하위 집합을 기반으로 4개의 LLM을 비교한다. 표 4에서 우리는 HYPERCLOVA X만이 CoT 프롬프팅으로 범주 전반에 걸쳐 성능을 안정적으로 향상시키는 반면, 다른 LLM은 종종 CoT로 성능 저하를 보인다는 것을 발견한다. 특히 GPT-3.5-TURBO와 GPT-4-TURBO는 STEM 및 응용과학에서 CoT로 더 나은 성능을 보이지만 HUMSS에서는 급격한 성능 하락을 보인다. 우리는 HUMSS 범주의 한국 특정 맥락이 다른 언어를 학습하여 일반화하기가 상대적으로 어려워 신뢰할 수 없는 설명을 초래한다고 추정한다 Turpin et al. (2023).

## 7. 결론

이 연구에서 우리는 번역된 내용 없이 원래 한국어 시험에서 가져온 45개 과목에 걸친 35,030개의 전문가 수준 객관식 문제로 구성된 포괄적인 편집인 KMMLU 벤치마크를 소개한다. 우리의 발견은 최첨단 LLM의 한국어 숙련도에서 상당한 개선 여지를 강조한다. 우리는 비한국어 LLM의 성능 향상이 한국어와 관련이 없는 능력에서 비롯된다는 것을 발견하여 한국 특정 맥락에서 더 나은 성능을 위한 한국어 사전학습의 중요성을 강조한다. 우리는 KMMLU 벤치마크가 연구자들이 현재 모델의 단점을 식별하는 데 도움이 되어 더 나은 한국어 LLM을 효과적으로 평가하고 개발할 수 있기를 기대한다.

## 8. 한계

광범위한 커버리지를 가진 벤치마크를 만들기 위해 최선을 다했지만, 향후 연구가 해결해야 할 몇 가지 한계가 있다. 첫째, 저작권 문제에 대한 우려로 인해 한국어, 의료 및 금융 도메인에서 상당한 수의 질문을 제거하여 커버리지 격차를 만들었다. 둘째, 채팅 정렬 LLM의 최근 급증은 생성 능력과 지시 따르기 기술을 평가하기 위한 전통적인 벤치마크의 효과에 의문을 제기했다. MMLU가 광범위한 지식을 평가하기 위한 사실상의 표준으로 계속 사용되고 있지만, LMSys Chatbot Arena(Zheng et al., 2023)와 같은 전용 LLM 심사위원 및 크라우드소싱 인간 선호도를 사용하여 생성 능력을 평가하는 추세가 변화하고 있다. 향후 노력은 생성 능력 평가를 포함하도록 한국어 벤치마킹 도구를 확장하는 것을 목표로 해야 한다. 또한 벤치마크의 잠재적 오용은 사회적 위험을 초래할 수 있다. 벤치마크에만 최적화하면 실제 응용 프로그램에서 성능이 좋지 않은 모델을 만들 수 있으므로 피해야 한다.

## 감사의 글

이 논문을 형성하고 알리는 데 도움이 된 대화에 대해 Zheng Xin Yong과 이유경에게 감사드린다. 또한 HyperCLOVA X 평가를 도와준 이재홍에게 감사드린다.